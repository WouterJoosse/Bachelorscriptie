In the last decades, technological progress has given rise to new developments in the field of data science. Computers are used extensively in our daily lives, and the amount of information available on the internet is ever increasing. This data can be used by universities, companies and governments to find patterns in human behaviour. Due to increased computing power of processors, increased storage capacities and programming techniques that are focussed around distributed computing, organizations more capable then ever to analyse this data. \\
This data is often stored in data warehouses, where each unit of data is stored in a structured way. Although data architecture practices describe some form of data normalization, most data warehouses are not necessarily compatible with each other and therefore most data concerning the same entity is often stored in a slightly different way across different data warehouses. More than ever, analysis of data often requires that different data sources should be joined in order to create more enriched data sets and to discover new links between data. \\

In order to link data sources with each other these data sources should be able to identify which entity in one data source can be coupled to the same entity in the other data source. However, data is not saved according to a universal standard and because of this, each source could save the same information in a different format, making it harder to link this data. \\ Humans usually are capable of deciding whether two data records refer to the same entity, because they have knowledge about the context and have acquired knowledge about the world.
However, matching data using humans will often require too much time and is therefore expensive, and it is often left to computers to do this task. The problem of combining records from the same data source, or from different data sources and finding data that are about the same entity is called \textit{Record Linkage}. \\

When the keys in two data sources do not correspond with each other, alternatives must be used to identify entities, and very often, the task of linking records becomes non-trivial. In the case of entity resolution of persons, this often boils down to personal information such as date of birth, addresses and names. These data are often prone to errors, such as typographical errors or variations, outdated information or the fact that some databases are not allowed to store certain information due to legislation. \\

Given these problems, record linkage techniques often use knowledge from several research fields. For example: in order to cope with typographical variations and errors, method developed by linguistics can be used, whereas the computer sciences can provide techniques to deal with large amounts of data in an efficient way. When a link has been established, statistical methods can be used to provide a certain degree of certainty about the correctness of a link. This places the field of record linkage in the domain of artificial intelligence, which aims to combine these reseach fields to provide a comprehensive method to model intelligent behaviour which, for instance, can be applied in computer software.

This report will focus on applying record linkage techniques on historical data. Historical data often brings an extra set of problems: most of the modern data modeling techniques where not applied, and often data is missing or stored in an incoherent method. 

\section{Overview of research on record linkage}
The term \textit{record linkage} was introduced by Dunn \cite{dunn1946record}, who wanted to assemble a `book of life' for each person which would describe the person's interaction with health and social security systems, and which would also contain the birth-, death- and marriage-certificates of that person. In 1969 Ivan Fellegi and Alan Sunter \cite{fellegi1969theory} proved that, when the attributes that are used in the comparison are independent of each other, a optimal probabilistic decision rule can be found. This work has been the cornerstone of much of the data matching systems that are used today. Their model classifies two records \textit{a} and \textit{b} as either a match, non-match or possible match, based on a decision function that computes the probabilities for each class. They suggested that the set of possible matches should be held under clerical review, in order to classify the records in that class as either a match or as non-match. Since this involves human decision making, which is not fail-safe.\\ 

\cite{newcombe1962record} introduces the concept of \textit{blocking} to the field of record linkage, by showing how to reduce the number of pairs to link to consider only those pairs that agree on some characteristics. \cite{levenshtein1966binary}, \cite{jaro1989advances} and \cite{winkler1990string} introduced methods to calculate string similarity. These techniques are widely used in automated matching algorithms.\\

In more recent years a lot of research has focused on using machine-learning algorithms for record linkage. \cite{sarawagi2002alias} described a method to use an active learning algorithm that minimizes human input when constructing similarity functions. \cite{winkler2002methods} used the Expectation-maximization algorithm and a bayesian network for record linkage in a unsupervised learning setting. \\

Examples of research that specifies on string matching for person names can be found in \cite{zobel1996phonetic}, who showed that more efficient matching on person names can be achieved when using phonetic forms of names. They introduced improvements on the standard Soundex (\cite{russell1918soundex}) to rewrite person names. \cite{bloothooft2014learning} also introduced rules to improve person name matching, based on the edit distance of the names when the names have been rewriten in semi-phonetic form. These rules to rewrite names into semi-phonetic form have been introduces by \cite{bloothooft1995rules}\\

In \cite{Aspects001} a method was proposed in order to link registration certificates of the life events from the \textit{Genlias} project. Since the persons in this data set are not provided with an unique id, matching of records is done by matching the names that appear on the certificates. This approach groups the certificates where the names of the persons that are similar to each other in an efficient way. Matches are found by calculating the edit distance between the names that occur on both certificates.\\

\section{The Genlias project}
Throughout history, governments held censi and civil status to identify civilians and keep records about them, in order to register taxations and the people that are allowed to vote, for example. So did the government of The Netherlands in the $18^{th}$ and $19^{th}$ century. During that time, The Netherlands was occupied by the Frence empire of Napolean Bonaparte. The French introduced the `Burgerlijke stand'; the civil status in the Netherlands, which was responsible for recording births, marriages and deaths of the inhabitants of the Netherlands. 

Census are often a very valuable source of information, as they show the state of the population at a given point in time. Civil registrations however provide more information about the development of the population, such as migration patterns and health statistics.

The \textit{Genlias} project is a project that has started in the last decade of the $20^{th}$ century with the aim to collect all the certificates that where produced into a single database. This database is available via \url{http://www.wiewaswie.nl}. \newline

The dataset gives us great insight into the population of the Netherlands in the $19^{th}$ century. However, unlike more modern registration practices, the original records form the civil status are not provided with unique identification numbers, which makes it harder to do research. Another complicating factor is that it was not uncommon for officials to write different names for the same person on different registrations. When people married, for example, the persons involved would be registered with the official names. However, when a person passed away, it could happen that the neighbors of that person had to report this event to the civil registration. These neighbors didn't always know the full official birth names of the person deceased.\\ 

Using record linkage to resolve the life events of the persons recorded in the Genlias dataset could give researchers invaluable information about the population of the Netherlands in the $19^{th}$ century. Examples of interest are infant mortality rates for health research or migration patterns and (social) mobility for economical and social research.

\section{An overview of the method}
The subject of this thesis is to find a method for matching names that have a high chance of spelling variations in the absence of a golden standard. In particular, by using the method proposed by \cite{Aspects001} and increasing the allowed edit distance, applying a set of rules on names of the generated matches and by applying checks that are generated from domain knowledge, this thesis will test if it is possible to be resistant to spelling variance in names and still generate reliable matches by checking if the resulting matches do correspond to knowledge about the real world. \\

The linkage of registrations is done in three phases. First, for each marriage certificate a set of potential matching certificates (targets) is build by matching the names of the bride and groom on each target certificate to the names of the parents on the candidate certificates, using the method that was described by \cite{Aspects001}. The candidate certificates consists of all the known certificates: all the birth, marriage and death certificates in the Genlias project for the province of Zeeland. In the case of a marriage registration, both the parent couples for the groom and bride are checked against a target certificate. 
The (levenshtein) edit distance can be used to regulate how strict the names should match each other.\\
Since the candidate registrations are always checked against the same target registrations of parents, the resulting links can be seen as a set of events in the lifes of the children of the parents that are mentioned on the target registration to which they are matched. The linking procedure will result in a mapping from a target registration to a set of candidate registrations. This set of registrations will be refered to as a `family'.\\

In the second phase, the four individual names on the links generated in the first phase are compared to each other. Links where the names differ too much are removed from the set. The third phase is to check the resulting matches against a set of rules, which are based on domain knowledge, in order to determine how well the registrations in the set of families can correspond to actual life events. For instance, it is very unlikely (if not impossible) for a child to be born fifty years after the father and mother have been married. The set of rules will filter out these links that are not consistent with a normal flow of life events.\\

The quality of the matching results will be evaluated based on the acceptance rate in the third phase. Since there is no golden standard available of true matches on the Genlias dataset, it is not possible to compute the regular performance standards, such as precision, recall or reduction ratio, as it is not clear what the matches and non-matches should be.